{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from torch import nn\n",
    "import sys, os\n",
    "\n",
    "# Add the 'modules/deepspeech/src/' directory to the system path\n",
    "sys.path.insert(0, os.path.abspath('../modules/deepspeech/src'))\n",
    "\n",
    "# from deepspeech.networks.utils import OverLastDim\n",
    "# from deepspeech.data import preprocess\n",
    "from torchvision.transforms import Compose\n",
    "# import torch.utils\n",
    "# import torch.utils.data\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')\n",
    "# ignore from matplotlib \n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from models.ds1 import DeepSpeech1WithContextFrames\n",
    "from ctc.ctc_loss_imp import *\n",
    "from data.librisubset import get_dataset_libri_sampled_loader, get_datapoint_i\n",
    "from utils.plot import *\n",
    "from utils.util import *\n",
    "from loss.loss  import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f441849a550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 237\u001b[0m\n\u001b[1;32m    229\u001b[0m     optimization_loop(inputs, x_param, output_sizes, target_sizes,\n\u001b[1;32m    230\u001b[0m                        optimizer, scheduler, net,\n\u001b[1;32m    231\u001b[0m                        dldw_targets \u001b[38;5;241m=\u001b[39m dldw_targets, params_to_match \u001b[38;5;241m=\u001b[39m  params_to_match, targets \u001b[38;5;241m=\u001b[39m targets, loss_func\u001b[38;5;241m=\u001b[39mloss_func,  FLAGS\u001b[38;5;241m=\u001b[39m FLAGS)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m## main\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m FLAGS \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mNamespace(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, optimizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m, reg_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, n_seeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m    238\u001b[0m                             n_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, drop_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_min_dur\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, batch_max_dur\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    240\u001b[0m exp_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/f006pq6/projects/asr-grad-reconstruction/logging/example_v2/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    241\u001b[0m exp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAGS\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_init_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAGS\u001b[38;5;241m.\u001b[39minit_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_opt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAGS\u001b[38;5;241m.\u001b[39moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAGS\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_reg_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAGS\u001b[38;5;241m.\u001b[39mreg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_regw_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAGS\u001b[38;5;241m.\u001b[39mreg_weight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_device_net(FLAGS):\n",
    "    device = 'cuda:0'\n",
    "    net = DeepSpeech1WithContextFrames(FLAGS.n_context, FLAGS.drop_prob).to(device)\n",
    "    return device, net\n",
    "\n",
    "# create a optimization loop function\n",
    "def optimization_loop(inputs, x_param, output_sizes, target_sizes,\n",
    "                       optimizer, scheduler, net, \n",
    "                       dldw_targets , params_to_match, targets, loss_func, FLAGS):\n",
    "\n",
    "\n",
    "    i=0\n",
    "    loss_history = []\n",
    "    loss_gm_history = []\n",
    "    loss_reg_history = []\n",
    "    stop_condition = False\n",
    "    while i < FLAGS.max_iter or not stop_condition:\n",
    "        # x_param_full= torch.concat([x_param, x_pad], dim=2)\n",
    "        out = net(x_param) # 1 176 29\n",
    "        out = out.log_softmax(-1)\n",
    "        # mloss, dldw_f = meta_loss(output, targets, output_sizes, target_sizes, dldw_target,  weight_param)\n",
    "        mloss, dldws = meta_loss(out, targets, None, None, dldw_targets,  params_to_match, loss_func)\n",
    "        gm_weight_distance = grad_distance(dldws[0], dldw_targets[0])\n",
    "        gm_bias_distance   = grad_distance(dldws[1], dldw_targets[1])\n",
    "\n",
    "        # regloss = tv_norm(x_param)\n",
    "        if FLAGS.reg == 'L2':\n",
    "            regloss = torch.norm(x_param, p=2)\n",
    "        elif FLAGS.reg == 'L1':\n",
    "            pass\n",
    "        elif FLAGS.reg == 'TV':\n",
    "            # need to make x_param from [n_frame, batch size, n_features] to [batch size, 1, n_features, n_frame]\n",
    "            regloss = tv_norm(x_param.permute(1,0,2).unsqueeze(1))\n",
    "        else:\n",
    "            regloss = torch.tensor(0.0)\n",
    "       \n",
    "        loss = (1-FLAGS.reg_weight)* mloss + FLAGS.reg_weight * regloss\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = x_param.grad.data\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(x_param, 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        ## PROJECT NON NEGATIVE\n",
    "        # x_param = x_param.clamp(min=0)\n",
    "        # with torch.no_grad():\n",
    "        #x_param.data = torch.clamp( x_param.data, min=0)\n",
    "\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        loss_gm_history.append(mloss.item() )\n",
    "        loss_reg_history.append(regloss.item() )\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            logging.info('Iter, Loss (A-G-Gw-Gb-R), Gradient Norm, Learning Rate: {:4d}, {:.8f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}'\\\n",
    "                        .format(i, loss.item(), mloss.item(),  gm_weight_distance.item(), gm_bias_distance.item(), regloss.item()\n",
    "            , grad.norm().item(), optimizer.param_groups[0][\"lr\"]))\n",
    "            # scheduler.step(mloss.item())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            plot_four_graphs(inputs.detach(), x_param.detach(), loss_history, loss_gm_history,loss_reg_history ,i, FLAGS)\n",
    "            pass\n",
    "            \n",
    "        \n",
    "        i+=1\n",
    "        # stet stop condition true if loss not decrease in last 100 iteration\n",
    "        if i>100 and loss_history[-1] > min(loss_history[-100:]):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            stop_condition\n",
    "    # save the reconstructed x_param, remember to detach and cpu it..\n",
    "    save_path = os.path.join(FLAGS.exp_path, 'x_param_last.pt')\n",
    "    torch.save(x_param.detach().cpu(), save_path)\n",
    "\n",
    "    return x_param\n",
    "\n",
    "# optimization_loop(x_param, optimizer, scheduler, net, dldw_target, weight_param, targets)\n",
    "\n",
    "# plot_mfcc(inputs.cpu().squeeze())\n",
    "\n",
    "# plot_mfcc(x_param.cpu().detach().squeeze())\n",
    "\n",
    "# plot_four_graphs(inputs.detach(), x_param.detach(), loss_history, loss_gm_history,loss_reg_history ,i)\n",
    "\n",
    "\n",
    "\n",
    "## write a main entry point for a python script file\n",
    "# that has args\n",
    "# 1. choose what is the index of the data point to reconstruct\n",
    "# 2. choose the learning rate\n",
    "# 3. choose what kind or regularization (L1, L2, TV)\n",
    "# 4. choose the weight of that regularization [0,1]\n",
    "# 5. choose the number of iterations\n",
    "# 6. choose number of seeds to try\n",
    "# python3 main.py --index 0 --lr 0.1 --reg L2 --reg_weight 0.05 --iterations 10000 --seeds 5\n",
    "# example of calling the main function with all args name\n",
    "# main(index=0, lr=0.1, reg='L2', reg_weight=0.05, iterations=1000, n_seeds=5)        \n",
    "\n",
    "\n",
    "def main(FLAGS):\n",
    "    \"\"\"\n",
    "    Main function for reconstructing data points with specified hyperparameters.\n",
    "   \n",
    "    Parameters:\n",
    "    - index: Index of the data point to reconstruct.\n",
    "    - lr: Learning rate for optimization.\n",
    "    - reg: Type of regularization ('L1', 'L2', 'TV').\n",
    "    - reg_weight: Weight of the regularization term.\n",
    "    - iterations: Number of iterations for the optimization.\n",
    "    \"\"\"\n",
    "    # Change all print statements to logging statements\n",
    "    logging.info('Reconstructing data point at index: {}'.format(FLAGS.index))\n",
    "    logging.info('Optimizer: {}'.format(FLAGS.optimizer_name))\n",
    "    logging.info('Learning rate: {}'.format(FLAGS.lr))\n",
    "    logging.info('Regularization: {}'.format(FLAGS.reg))\n",
    "    logging.info('Regularization weight: {}'.format(FLAGS.reg_weight))\n",
    "    logging.info('Number of iterations: {}'.format(FLAGS.iterations))\n",
    "\n",
    "    #check if exp_path exists or create\n",
    "    if not os.path.exists(FLAGS.exp_path):\n",
    "        os.makedirs(FLAGS.exp_path)\n",
    "        logging.info('exp_path {} created'.format(FLAGS.exp_path))\n",
    "    if not os.path.exists(os.path.join(FLAGS.exp_path, 'figures')):\n",
    "        os.makedirs(os.path.join(FLAGS.exp_path, 'figures'))    \n",
    "    logging.info('logging experiment to {}'.format(FLAGS.exp_path))\n",
    "\n",
    "    device, net = get_device_net(FLAGS)\n",
    "    logging.info('Device: {}'.format(device))\n",
    "    logging.info('Network: {}'.format((net.__class__.__name__)))\n",
    "\n",
    "\n",
    "    # check if example/net_params.pt exists, if not create by saving the net state_dict, if yes then load\n",
    "    if not os.path.exists(os.path.join(FLAGS.exp_path, 'net_params.pt')):\n",
    "        torch.save(net.state_dict(), os.path.join(FLAGS.exp_path, 'net_params.pt'))\n",
    "        logging.info('net_params.pt created')\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(os.path.join(FLAGS.exp_path, 'net_params.pt')))\n",
    "        logging.info('net_params.pt loaded')\n",
    "\n",
    "     # get device net dataset loader datapoint i\n",
    "    if FLAGS.index != 0:\n",
    "        raise ValueError('script now run for index 0 only')\n",
    "\n",
    "    # check if input.pt exists, if not create by loading the next_item and save it\n",
    "    # if not os.path.exists(os.path.join(FLAGS.exp_path, 'input.pt')):\n",
    "    #     dataset, loader = get_dataset_loader(net)\n",
    "    #     next_item = get_datapoint_i(iter(loader), 0)\n",
    "    #     torch.save(next_item, os.path.join(FLAGS.exp_path, 'input.pt'))\n",
    "    #     logging.info('input.pt created')\n",
    "    # else:\n",
    "    #     next_item = torch.load(os.path.join(FLAGS.exp_path, 'input.pt'))\n",
    "    #     logging.info('input.pt loaded')\n",
    "    dataset, loader = get_dataset_libri_sampled_loader(net, FLAGS)\n",
    "    next_item = get_datapoint_i(iter(loader), 0)\n",
    "\n",
    "    logging.info('')\n",
    "\n",
    "\n",
    "    inputs ,input_sizes = next_item[0]\n",
    "    logging.info('inputs mean and std: {}, {}'.format(inputs.mean(), inputs.std()))\n",
    "    # input_sizes is tensor list of inputs.shape[1] elements with value inputs.shape[0]\n",
    "\n",
    "    targets = next_item[1]\n",
    "    target_sizes = torch.Tensor([len(t) for t in targets]).int()\n",
    "\n",
    "    #target is list of tensor with different length, pad it to the same length in a tensor\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    # transfer the data to the GPU\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.long().to(device)\n",
    "\n",
    "    input_sizes = input_sizes.long().to(device)\n",
    "    target_sizes = target_sizes.long().to(device)\n",
    "\n",
    "    # get the target gradient\n",
    "    # param to match, a list of pointer to params\n",
    "    params_to_match = [net.network.out.module[0].weight, net.network.out.module[0].bias]\n",
    "    out = net(inputs)\n",
    "    output_sizes = (torch.ones(out.shape[1]) * out.shape[0]).int()\n",
    "    # output_sizes = torch.Tensor([out.size(0)]).int().to(device)\n",
    "    out =  out.log_softmax(-1)\n",
    "\n",
    "    # loss_func = lambda x,y :batched_ctc_logspace_scale(x, y, output_sizes, target_sizes)\n",
    "    loss_func = lambda x,y :ctc_loss_imp(x, y, output_sizes, target_sizes)\n",
    "    # loss_func = lambda x,y :batched_ctc_logspace_scale(x, y, output_sizes, target_sizes)\n",
    "    loss_func_lib   = torch.nn.CTCLoss()\n",
    "    loss = loss_func(out, targets)\n",
    "    loss_lib = loss_func_lib(out.cpu(), targets.cpu(), output_sizes.cpu(), target_sizes.cpu())\n",
    "    logging.debug('loss: {}'.format(loss.item()))\n",
    "    logging.debug('loss by pt lib: {}'.format(loss_lib.item()))\n",
    "    dldw_targets = torch.autograd.grad(loss, params_to_match)\n",
    "\n",
    "    ## zero out small values keep 10% largest dldw_target\n",
    "    # logging.info('zero out small values keep 10% largest dldw_target')\n",
    "    # dldw_target = dldw_target * (dldw_target.abs() > dldw_target.abs().topk(int(0.1*dldw_target.numel()))[0][-1])\n",
    "    for ip, p in enumerate(params_to_match):\n",
    "        p.requires_grad = True\n",
    "        logging.debug('matching {}. params with shape {} and norm {} first ten {}'.format(ip, p.shape, p.norm(), p.flatten()[:10]))\n",
    "        logging.debug('                    gradient norm {}'.format(dldw_targets[ip].norm()))\n",
    "\n",
    "    # init x_param\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    x_init =  init_a_point(inputs, FLAGS)\n",
    "    x_param = torch.nn.Parameter(x_init.to(device),requires_grad=True)\n",
    "\n",
    "\n",
    "    if FLAGS.optimizer_name.lower() == 'adam':\n",
    "        optimizer = optim.Adam([x_param], lr=FLAGS.lr)\n",
    "    elif FLAGS.optimizer_name.lower() == 'sgd':\n",
    "        optimizer = optim.SGD([x_param], lr=FLAGS.lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {FLAGS.optimizer_name}\")\n",
    "\n",
    "    # reduce lr at epoch 250, 500, 750 half\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=list(range(250,2000,250)), gamma=0.5)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=.5,patience=50)\n",
    "\n",
    "\n",
    "    # suggest an experiment name base on datapoint index, optimizer name,  learning rate, regularizer, regularizer weight\n",
    "    logging.info('Experiment Name: {}'.format(os.path.basename(FLAGS.exp_path)))\n",
    "\n",
    "    optimization_loop(inputs, x_param, output_sizes, target_sizes,\n",
    "                       optimizer, scheduler, net,\n",
    "                       dldw_targets = dldw_targets, params_to_match =  params_to_match, targets = targets, loss_func=loss_func,  FLAGS= FLAGS)\n",
    "\n",
    "\n",
    "## main\n",
    "\n",
    "\n",
    "FLAGS = argparse.Namespace(index=0, optimizer_name='Adam', lr=0.5, reg='None', reg_weight=0, iterations=10000, n_seeds=5, max_iter=10000,\n",
    "                            n_context=6, drop_prob=0, init_method='same', batch_min_dur=2000, batch_max_dur=3000, batch_size=1)\n",
    "\n",
    "exp_path='/scratch/f006pq6/projects/asr-grad-reconstruction/logging/example_v2/'\n",
    "exp_name = f\"idx_{FLAGS.index}_init_{FLAGS.init_method}_opt_{FLAGS.optimizer_name}_lr_{FLAGS.lr}_reg_{FLAGS.reg}_regw_{FLAGS.reg_weight}\"\n",
    "FLAGS.exp_path=os.path.join(exp_path, exp_name)\n",
    "\n",
    "main(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr-grad-reconstruction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
